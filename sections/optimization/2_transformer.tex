\subsection[ft-transformer]{\gls{ftt}}

We adopt the \gls{ftt} proposed in \cite{ft-transformer} architecture, which uses the transformer \cite{transformer}
architecture at its core.

\subsubsection{Transformer architecture}

The transformer architecture was originally proposed for natural language processing tasks, but has since been applied
to almost every domain of machine learning. The transformer architecture is a fully-attentional architecture, which
means that it does not use any convolutional \cite{convolutional} or recurrent \cite{recurrent} layers. Instead, it uses
the attention mechanism to learn the dependencies between the input features. The transformer architecture is composed
of multiple blocks, each consisting of a \verb|MultiHeadAttention| layer followed by a simple \verb|FeedForward| layer.
The \verb|MultiHeadAttention| layer is composed of multiple \verb|Attention| heads, which are then concatenated and
projected to the desired dimensionality. The \verb|FeedForward| consists of two \verb|Linear| layers with a \verb|GELU|
activation in-between. To improve and stabilize the training \verb|LayerNorm| \cite{layernorm} layers are used. And
residual connections are introduced to improve with the training of deep networks.

\subsubsection{Post-norm vs pre-norm formulation}

Since the original paper \cite{transformer}, not many things have changed with the transformer design. The notable
change is that \verb|LayerNorm| layer has been moved from after the \verb|Attention| and \verb|FeedForward| layers
(post-norm formulation) to before them (pre-norm formulation). The pre-norm formulation has been shown to be more
stable and easier to train \cite{pre-norm}. We also adopt this change, following \cite{ft-transformer}.

Aside from the introduced handling of the missing/invalid values, there is one last difference from the original
\gls{ftt}: instead of applying the \verb|MultiHeadAttention| layer in the end of the whole stack of blocks to obtain
the logits, we use a fully-connected \verb|Linear| layer. I have no idea why I did this. The whole structure is shown
on the \autoref{fig:ftt}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ftt.pdf}
    \caption{\gls{ftt} architecture.}
    \label{fig:ftt}
\end{figure}
